{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8NoPbwr+suRheWAPKG2k3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yule-li/colab-hub/blob/main/crawl4ai_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/unclecode/crawl4ai.git\n",
        "%cd crawl4ai\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ggUj0HIP9M",
        "outputId": "cda3dfef-15b8-44a0-ba32-bde4af2e9cd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'crawl4ai'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 111 (delta 56), reused 84 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (111/111), 39.66 KiB | 1.59 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "/content/crawl4ai\n",
            "Obtaining file:///content/crawl4ai\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi (from Crawl4AI==0.1.0)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from Crawl4AI==0.1.0)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting selenium (from Crawl4AI==0.1.0)\n",
            "  Downloading selenium-4.20.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from Crawl4AI==0.1.0) (2.7.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from Crawl4AI==0.1.0) (3.9.5)\n",
            "Collecting aiosqlite (from Crawl4AI==0.1.0)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Collecting chromedriver_autoinstaller (from Crawl4AI==0.1.0)\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Collecting httpx (from Crawl4AI==0.1.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Crawl4AI==0.1.0) (2.31.0)\n",
            "Collecting bs4 (from Crawl4AI==0.1.0)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting html2text (from Crawl4AI==0.1.0)\n",
            "  Downloading html2text-2024.2.26.tar.gz (56 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting litellm (from Crawl4AI==0.1.0)\n",
            "  Downloading litellm-1.37.2-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from Crawl4AI==0.1.0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Crawl4AI==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Crawl4AI==0.1.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Crawl4AI==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Crawl4AI==0.1.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Crawl4AI==0.1.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Crawl4AI==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiosqlite->Crawl4AI==0.1.0) (4.11.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->Crawl4AI==0.1.0) (4.12.3)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver_autoinstaller->Crawl4AI==0.1.0) (24.0)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->Crawl4AI==0.1.0) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->Crawl4AI==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->Crawl4AI==0.1.0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->Crawl4AI==0.1.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->Crawl4AI==0.1.0) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->Crawl4AI==0.1.0) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->Crawl4AI==0.1.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->Crawl4AI==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->Crawl4AI==0.1.0) (2.18.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm->Crawl4AI==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm->Crawl4AI==0.1.0) (7.1.0)\n",
            "Collecting openai>=1.0.0 (from litellm->Crawl4AI==0.1.0)\n",
            "  Downloading openai-1.28.0-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken>=0.4.0 (from litellm->Crawl4AI==0.1.0)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm->Crawl4AI==0.1.0) (0.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Crawl4AI==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Crawl4AI==0.1.0) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium->Crawl4AI==0.1.0)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium->Crawl4AI==0.1.0)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm->Crawl4AI==0.1.0) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi->Crawl4AI==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->litellm->Crawl4AI==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm->Crawl4AI==0.1.0) (4.66.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->Crawl4AI==0.1.0) (1.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->litellm->Crawl4AI==0.1.0) (2023.12.25)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->Crawl4AI==0.1.0) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium->Crawl4AI==0.1.0)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->Crawl4AI==0.1.0)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3<3,>=1.21.1->requests->Crawl4AI==0.1.0) (1.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->Crawl4AI==0.1.0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn->Crawl4AI==0.1.0) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->Crawl4AI==0.1.0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->Crawl4AI==0.1.0)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn->Crawl4AI==0.1.0)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->Crawl4AI==0.1.0) (2.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm->Crawl4AI==0.1.0) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm->Crawl4AI==0.1.0) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm->Crawl4AI==0.1.0) (2023.6.0)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->Crawl4AI==0.1.0)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->Crawl4AI==0.1.0) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->Crawl4AI==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->Crawl4AI==0.1.0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->Crawl4AI==0.1.0) (0.1.2)\n",
            "Building wheels for collected packages: html2text\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html2text: filename=html2text-2024.2.26-py3-none-any.whl size=33111 sha256=db068a044fb04fedfb2956eb99208f12bc97ec0ed04b4b7f86b6e8f3c4db1cc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/96/6d/a7eba8f80d31cbd188a2787b81514d82fc5ae6943c44777659\n",
            "Successfully built html2text\n",
            "Installing collected packages: websockets, uvloop, ujson, shellingham, python-multipart, python-dotenv, outcome, orjson, httptools, html2text, h11, dnspython, chromedriver_autoinstaller, aiosqlite, wsproto, watchfiles, uvicorn, trio, tiktoken, starlette, httpcore, email_validator, bs4, typer, trio-websocket, httpx, selenium, openai, litellm, fastapi-cli, fastapi, Crawl4AI\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Running setup.py develop for Crawl4AI\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Crawl4AI aiosqlite-0.20.0 bs4-0.0.2 chromedriver_autoinstaller-0.6.4 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 h11-0.14.0 html2text-2024.2.26 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 litellm-1.37.2 openai-1.28.0 orjson-3.10.3 outcome-1.3.0.post0 python-dotenv-1.0.1 python-multipart-0.0.9 selenium-4.20.0 shellingham-1.5.4 starlette-0.37.2 tiktoken-0.6.0 trio-0.25.0 trio-websocket-0.11.1 typer-0.12.3 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install chromium-browser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwtUxwn9H6JJ",
        "outputId": "ddac2852-f20f-4ef0-f47c-99cd5c201c12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 28.3 MB of archives.\n",
            "After this operation, 117 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.62+22.04 [25.8 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.3 MB in 1s (30.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 122126 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.62+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.62+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service â†’ /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.62+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service â†’ /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service â†’ /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service â†’ /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service â†’ /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service â†’ /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service â†’ /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service â†’ /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer â†’ /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket â†’ /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service â†’ /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 122356 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.web_crawler import WebCrawler\n",
        "from crawl4ai.models import UrlModel\n",
        "from crawl4ai.utils import get_content_of_website\n",
        "import os\n",
        "OPENAI_API_KEY=''\n",
        "def main():\n",
        "    # Initialize the WebCrawler with just the database path\n",
        "    crawler = WebCrawler(db_path='crawler_data.db')\n",
        "\n",
        "    # Fetch a single page\n",
        "    single_url = UrlModel(url='https://www.nbcnews.com/business', forced=False)\n",
        "    result = crawler.fetch_page(\n",
        "        single_url,\n",
        "        provider= \"openai/gpt-3.5-turbo\",\n",
        "        api_token = OPENAI_API_KEY,\n",
        "        extract_blocks_flag=True,\n",
        "        word_count_threshold=10\n",
        "    )\n",
        "    # print(result.model_dump())\n",
        "    # print(result.markdown)\n",
        "    # print(result.cleaned_html)\n",
        "    print(result.parsed_json)\n",
        "\n",
        "    # Fetch multiple pages\n",
        "    # urls = [\n",
        "    #     UrlModel(url='http://example.com', forced=False),\n",
        "    #     UrlModel(url='http://example.org', forced=False)\n",
        "    # ]\n",
        "    # results = crawler.fetch_pages(urls, provider= \"openai/gpt-4-turbo\", api_token = os.getenv('OPENAI_API_KEY'))\n",
        "    # for res in results:\n",
        "    #     print(res.model_copy())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGTLHAscGw4m",
        "outputId": "e9c72635-1cc8-48d0-e864-6f54298f83bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling is done ðŸš€\n",
            "[{\"index\": 0, \"tags\": [\"opt-out\"], \"content\": [\"To opt out of the use of your email and other personal information related to that email such as your name for targeted advertising activities please complete this\"], \"error\": false}, {\"index\": 1, \"tags\": [\"cookie-notice\"], \"content\": [\"Please see our Cookie Notice for more details which can be found by navigating to the Privacy Policy in the menu settings page.\"], \"error\": false}, {\"index\": 0, \"tags\": [\"news\"], \"content\": [\"The US Consumer Financial Protection Bureau headquarters.\", \"The US Consumer Financial Protection Bureau headquarters.\", \"Hyundai and Kia unit settles U.S. charges it repossessed service members vehicles\", \"Hyundai was accused of taking over 26 vehicles whose owners had begun paying off their loans prior to active duty.\", \"The Hyundai Motor Co. logo is seen on the front grill of a Santa Fe sports utility vehicle\", \"The Hyundai Motor Co. logo is seen on the front grill of a Santa Fe sports utility vehicle\"], \"error\": false}, {\"index\": 1, \"tags\": [\"news\"], \"content\": [\"Professor seen on video verbally attacking woman in hijab will no longer teach at Arizona State\", \"Novak Djokovic accidentally hit in head by water bottle at Italian Open\", \"French official disputes passage about Emmanuel Macron in Kristi Noems book\", \"An asylum-seeker who died in ICE custody was sick for months. Advocates fear other ill people are not getting proper care.\", \"Brown Girl Therapy founder tackles South Asian mental health stigma in new book\", \"Two slayings rocked Atlantas hip-hop scene a decade ago. Only one has been solved.\", \"Eurovisions LGBTQ contestants make their mark in a turbulent year for the contest\", \"Farmers will now get paid to test their dairy cows for bird flu\", \"Barron Trump declines invitation to be a delegate at the Republican National Convention\", \"AI personas are the future of dating, Bumble founder says. Many arent buying.\", \"A small summer Covid wave may be ahead, experts say, as a new variant takes over\", \"Lesbian and bisexual women die earlier than straight women, decadeslong study finds\", \"Biden faces his next campus test over Gaza: From the Politics Desk\", \"Bidens arms threat to Israel better than nothing but too late, say U.S. officials who resigned over Gaza policy\", \"Police dismantle encampments on college campuses across U.S. as graduations approach\", \"Kylian Mbapp\\u00e9 makes official what has long been rumored: Hes leaving Paris Saint-Germain\", \"Woman found living in Michigan grocery store sign, complete with computer and Keurig, for months\", \"Hawaiis governor says most residents displaced by wildfire now have long-term housing\", \"Pregnant bride gets married in hospital wearing wedding dress of bed sheets\", \"Flavor Flav is the official hype man for the U.S. womens water polo team in the Paris Olympics\"], \"error\": false}, {\"index\": 2, \"tags\": [\"privacy\"], \"content\": [\"Residents of California, Connecticut, Colorado, Utah or Virginia we have received your Global Privacy Control signal or you have opted out from the toggle below, but there is another step. To opt out of us selling or sharing/processing data such as your name, email address and other associated personal information for targeted advertising activities as described above, please submit the form below. ALL OTHER LOCATIONS: If we do not detect that you are in California, Connecticut, Colorado, Utah or Virginia, these choices will not apply even if you toggle this button off. Your Privacy Choices: Opt-out of sale of personal information and Opt-out of sharing or processing personal information for targeted ads To provide you with a more relevant online experience, certain online ad partners may combine personal information that we make available with data across different businesses and otherwise assist us with related advertising activities, as described in our . This may be considered \\\"selling\\\" or \\\"sharing/processing for targeted online advertising under applicable law. If you are a resident of California, Connecticut, Colorado, Utah or Virginia, to opt out of us selling or sharing/processing your personal information: Such as cookies and devices identifiers for the targeted ads and related purposes for this site/app on this browser/device: switch the Allow Sale of My Personal Info or Sharing/Processing for Targeted Ads toggle under Manage Preferences to OFF (grey color) by moving it LEFT and clicking Confirm My Choice. Such as your name, email address and other associated personal information for targeted advertising activities as described above, please submit the form below. Please note that choices related to cookies and device identifiers are specific to the brands website or app on the browser or device where you are making the election. Allow Sale of My Personal Info and Sharing/Processing for Targeted Ads Allow Sale of My Personal Info and Sharing/Processing for Targeted Ads To opt out of selling or sharing/processing for targeted advertising of information such as cookies and device identifiers processed for targeted ads (as defined by law) and related purposes for this site/app on this browser/device, switch this toggle to off (grey color) by moving it left and clicking Confirm My Choice below. (This will close this dialogue box, so please open the email Opt-Out Form 1st). If we do not detect that you are in this choice will not apply even if you toggle this button off.If you turn this off, you will still see ads, but they may be less relevant or based only on our first-party information about you.Please note, you must make the Manage Preference choices on each site/app on each browser/device you use to access the services. You must also renew this choice if you clear your cookies. You can change your precise geolocation permissions for our mobile apps in your mobile device settings.\"], \"error\": false}, {\"index\": 0, \"tags\": [\"news\"], \"content\": [\"IE 11 is not supported. For an optimal experience visit our site on another browser.\"], \"error\": false}, {\"index\": 1, \"tags\": [\"news\"], \"content\": [\"My News\"], \"error\": false}, {\"index\": 2, \"tags\": [\"news\"], \"content\": [\"Sign In\"], \"error\": false}, {\"index\": 3, \"tags\": [\"news\"], \"content\": [\"Electric vehicles lined up.\", \"Biden administration plans to raise tariffs on electric vehicles from China\"], \"error\": false}, {\"index\": 4, \"tags\": [\"news\"], \"content\": [\"McDonalds store sign\", \"Police confront environmental activists near the Tesla Gigafactory electric car factory\", \"Climate protesters try to break into Teslas Germany factory, multiple people arrested\"], \"error\": false}, {\"index\": 5, \"tags\": [\"news\"], \"content\": [\"Pride month merchandise inside the store\", \"Target says Pride collection will appear in select stores, cuts LGBTQ apparel for kids\"], \"error\": false}, {\"index\": 6, \"tags\": [\"news\"], \"content\": [\"Fatburger logo.\", \"Seal of the U.S. Social Security Administration\", \"Social Security Administration to expand access to certain benefits through several upcoming changes\"], \"error\": false}, {\"index\": 7, \"tags\": [\"news\"], \"content\": [\"Israeli attacks continue in Gaza Strip\", \"Biden administration finds Israel may have violated international law, but not a U.S. weapons agreement\"], \"error\": false}, {\"index\": 8, \"tags\": [\"news\"], \"content\": [\"Joe Biden speaking\", \"Morehouse faculty set to vote on awarding Biden an honorary degree amid commencement opposition\"], \"error\": false}, {\"index\": 9, \"tags\": [\"news\"], \"content\": [\"Michael Cohen inside the courthouse\", \"Trump will face Michael Cohen in court as prosecution nears the end of its case\"], \"error\": false}, {\"index\": 10, \"tags\": [\"news\"], \"content\": [\"Northern lights over Baden-W\\u00fcrttemberg\", \"Valentin Gensch / dpa / picture alliance via Getty Images\", \"Severe solar storm expected to supercharge northern lights across the U.S.\"], \"error\": false}, {\"index\": 11, \"tags\": [\"news\"], \"content\": [\"Federal regulators to hold hearing on airline credit card rewards\", \"Media executive Bonnie Hammer speaks about women in the workplace\", \"What is spaving and how can you avoid this financial trap?\", \"Sam Altman takes nuclear energy company Oklo public to help power his AI ambitions\", \"Oklos business model is based on commercializing nuclear fission, the reaction that fuels all nuclear power plants.\", \"Sam Altman, chief executive officer of OpenAI, in Atlanta on Dec. 11, 2023.\"], \"error\": false}, {\"index\": 12, \"tags\": [\"news\"], \"content\": [\"PR executive at Chinese tech firm Baidu apologizes for comments seen as glorifying overwork\", \"Baidus head of communications, Qu Jing, drew a public outcry after she implied that she was not concerned about her employees and said she only cared about results.\", \"A top public relations executive from Chinese technology firm Baidu apologized Thursday after she made comments in a series of videos that critics said glorified a culture of overwork.\"], \"error\": false}, {\"index\": 13, \"tags\": [\"news\"], \"content\": [\"Senate passes sweeping FAA bill focused on safety and consumer protections\", \"The bill, which the House is expected to pass next week, would prohibit airlines from charging fees for families to sit together.\"], \"error\": false}, {\"index\": 14, \"tags\": [\"news\"], \"content\": [\"Apple apologizes for crushed iPad Pro ad after widespread online blowback\", \"\\\"We missed the mark,\\\" Apple Vice President Tor Myhren said in a statement obtained by Ad Age.\", \"The Apple store in the Americana at Brand shopping center in Glendale, Calif.\"], \"error\": false}, {\"index\": 15, \"tags\": [\"news\"], \"content\": [\"Consumers might be paying more than credit card perks are worth, officials say\", \"Fees could overshadow the promise of travel rewards, the Biden administration says as it scrutinizes frequent flyer and other loyalty programs.\", \"A traveler walks past a sign advertising a Delta Air Lines credit card at Seattle-Tacoma International Airport in 2015\"], \"error\": false}, {\"index\": 16, \"tags\": [\"news\"], \"content\": [\"Sinclair explores selling roughly 30 of its broadcast stations, sources say\", \"Sinclair owns or operates 185 TV stations in 86 markets. The stations are affiliated with several networks, including Fox, NBC, ABC, CBS and the CW.\", \"Sinclair Broadcast Group Inc. Headquarters As Tribune Media Co. Deal Falls Through\"], \"error\": false}, {\"index\": 17, \"tags\": [\"news\"], \"content\": [\"The Federal Trade Commission charged BetterHelp with using sensitive customer information for advertising purposes without properly getting their permission to do so.\", \"The Federal Trade Commission headquarters sign in the midst of tree branches and leaves\", \"Bruce Garelick had been on the board of the publicly traded company shell company, which eventually merged with the owner of the Truth Social app.\", \"Image: Bruce Garelick\"], \"error\": false}, {\"index\": 18, \"tags\": [\"news\"], \"content\": [\"The U.S. surgeon general says separating smoking and nonsmoking sections, cleaning the air and ventilation arent effective protections against secondhand smoke.\", \"A visitor plays a slot machine with others nearby at a casino\"], \"error\": false}, {\"index\": 19, \"tags\": [\"news\"], \"content\": [\"CFPB rule to save Americans 10 billion a year in late fees faces possible last-minute freeze\", \"Led by the U.S. Chamber of Commerce, the industry in March sued the Consumer Financial Protection Bureau in federal court to prevent the new rule from taking effect.\"], \"error\": false}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eomyM-NDILPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}